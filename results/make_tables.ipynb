{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures for Frame et al. 2022 \"On mass conservation...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import xarray as xr\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import metrics\n",
    "import random\n",
    "import scipy.stats as st\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.metrics import mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm2inch(width: float, height: float) -> tuple:\n",
    "    \"\"\"Converts figure size from centimeters to inch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    width : float\n",
    "        Width of figure in centimeters\n",
    "    height : float\n",
    "        Height of figure in centimeters\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float]\n",
    "        Tuple containing width and height in inches\n",
    "    \"\"\"\n",
    "    inch = 2.54\n",
    "    return (width / inch, height / inch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert flow to   CFS mm -> ft     km^2 -> ft^2    hr->s\n",
    "conversion_factor = 0.00328084 * 10763910.41671 / 3600 / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camels attributes with RI information\n",
    "dataName = '../data/camels_attributes.csv'\n",
    "# load the data with pandas\n",
    "pd_attributes = pd.read_csv(dataName, sep=',', index_col='gauge_id')\n",
    "\n",
    "# Add the basin ID as a 8 element string with a leading zero if neccessary\n",
    "basin_id_str = []\n",
    "for a in pd_attributes.index.values:\n",
    "    basin_id_str.append(str(a).zfill(8))\n",
    "pd_attributes['basin_id_str'] = basin_id_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_metrics = ['NSE','KGE','Pearson-r','Alpha-NSE','Beta-NSE']\n",
    "loop_these_metrics = ['NSE','KGE','Pearson-r','Alpha-NSE','Beta-NSE', 'mi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_MI(x, y, bins):\n",
    "    c_xy = np.histogram2d(x, y, bins)[0]\n",
    "    mi = mutual_info_score(None, None, contingency=c_xy)\n",
    "    return mi\n",
    "\n",
    "def calculate_all_metrics_for_frequency_analysis(analysis_dict, flows):\n",
    "    \n",
    "    sims = list(flows.keys())[:-1]\n",
    "\n",
    "    for metric in loop_these_metrics:\n",
    "\n",
    "        score = {sim:0 for sim in sims}\n",
    "        \n",
    "        if metric == 'NSE':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.nse(flows['obs'],flows[sim])\n",
    "        if metric == 'MSE':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.mse(flows['obs'],flows[sim])\n",
    "        if metric == 'RMSE':\n",
    "            for sim in sims:\n",
    "                 score[sim] = metrics.rmse(flows['obs'],flows[sim])\n",
    "        if metric == 'KGE':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.kge(flows['obs'],flows[sim])\n",
    "        if metric == 'KGEss':\n",
    "            for sim in sims:\n",
    "                score[sim] = (metrics.kge(flows['obs'],flows[sim])+0.41421356237)/np.sqrt(2)\n",
    "        if metric == 'Alpha-NSE':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.alpha_nse(flows['obs'],flows[sim])\n",
    "        if metric == 'Beta-NSE':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.beta_nse(flows['obs'],flows[sim])\n",
    "        if metric == 'Pearson-r':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.pearsonr(flows['obs'],flows[sim])\n",
    "        if metric == 'Peak-Timing':\n",
    "            for sim in sims:\n",
    "                score[sim] = np.abs(metrics.mean_peak_timing(flows['obs'],flows[sim]))\n",
    "        if metric == 'FHV':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.fdc_fhv(flows['obs'],flows[sim])          \n",
    "        if metric == 'FLV':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.fdc_flv(flows['obs'],flows[sim])          \n",
    "        if metric == 'FMS':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.fdc_fms(flows['obs'],flows[sim])   \n",
    "                \n",
    "        if metric == \"mi\":\n",
    "            for sim in sims:\n",
    "                score[sim] = calc_MI(flows['obs'],flows[sim], 100)\n",
    "\n",
    "          \n",
    "        for sim in sims:\n",
    "            analysis_dict[metric][sim].append(score[sim])\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------\n",
    "# Standard Time Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test period(1989-1999) is the same period used by previous studies  which allows us to confirm that the deep learning models (LSTM andMC-LSTM) trained for this project perform as expected relative to prior work. \n",
    "These metrics are broadly equivalent to thosereported for single models (not ensembles) by (Kratzert et al., 2019c) (LSTM) and (Hoedt et al., 2021) (MC-LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Time Split\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------\n",
    "# Set up lists to use in loops\n",
    "models =        ['lstm','mc', 'sac']\n",
    "flows =         ['lstm','mc', 'sac', 'obs']\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "calculate_metrics_now = False\n",
    "if calculate_metrics_now:\n",
    "\n",
    "    lstm_results_time_split1   = {}\n",
    "    mclstm_results_time_split1 = {}\n",
    "    sacsma_results_time_split1 = {}\n",
    "    \n",
    "    for forcing_type in ['nldas', 'daymet']:\n",
    "\n",
    "        with open('./model_output_for_analysis/lstm_time_split1_{}_ens.p'.format(forcing_type), 'rb') as fb:\n",
    "            lstm_results_time_split1[forcing_type] = pkl.load(fb)\n",
    "        with open('./model_output_for_analysis/mclstm_time_split1_{}_ens.p'.format(forcing_type), 'rb') as fb:\n",
    "            mclstm_results_time_split1[forcing_type] = pkl.load(fb)\n",
    "        with open('./model_output_for_analysis/sacsma_time_split1_{}_ens.p'.format(forcing_type), 'rb') as fb:\n",
    "            sacsma_results_time_split1[forcing_type] = pkl.load(fb)\n",
    "\n",
    "    basin_list = lstm_results_time_split1[forcing_type].keys()\n",
    "\n",
    "    st1_dict = {}\n",
    "    st1_dict_stackz = {forcing_type:{flow:[] for flow in flows} for forcing_type in ['nldas', 'daymet']}\n",
    "\n",
    "    for forcing_type in ['nldas', 'daymet']:\n",
    "\n",
    "        analysis_dict_temp_large = {}\n",
    "        #-------------------------------------------------------------------------------------------------            \n",
    "\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "        #-----LOOP THROUGH BASINS------------------------------------------------------------------------\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "        for ib, basin_0str in enumerate(basin_list): \n",
    "            basin_int = int(basin_0str)\n",
    "\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Setting up the dictionary for the single basin results. Then will add to the overall dict.\n",
    "            analysis_dict_temp_small = {metric:{model:[] for model in models} for metric in loop_these_metrics}\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # We need the basin area to convert to CFS, to interpolate the RI from LPIII\n",
    "            basin_area = pd_attributes.loc[basin_int, 'area_geospa_fabric']\n",
    "            basin_str = str(basin_int).zfill(8)\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            #----  Set the time period for metrics   ---------------------------------------------------------\n",
    "            date_from = '1989-10'\n",
    "            date_to = '1999-09'\n",
    "            #-------------------------------------------------------------------------------------------------    \n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Make dictionary with all the flows\n",
    "            flow_mm = {}\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Standard LSTM data trained on all years\n",
    "            xrr = lstm_results_time_split1[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_sim']\n",
    "            flow_mm['lstm'] = pd.DataFrame(data=xrr.values,index=xrr.datetime.values).loc[date_from:date_to]\n",
    "            #-------------------------------------------------------------------------------------------------        \n",
    "            # Mass Conserving LSTM data\n",
    "            xrr = mclstm_results_time_split1[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_sim']\n",
    "            flow_mm['mc'] = pd.DataFrame(data=xrr.values,index=xrr.datetime.values).loc[date_from:date_to]\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # SACSMA Sinlge run\n",
    "            df = sacsma_results_time_split1[forcing_type][basin_0str]\n",
    "            flow_mm['sac'] = df.loc[date_from:date_to]\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # OBSERVATIONS\n",
    "            xrr = mclstm_results_time_split1[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_obs']\n",
    "            flow_mm['obs'] = pd.DataFrame(data=xrr.values,index=xrr.datetime.values).loc[date_from:date_to]\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "            for flow in flows:\n",
    "                st1_dict_stackz[forcing_type][flow].extend(list(flow_mm[flow].values))\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Make all xarray data similar\n",
    "            for iflow in flows:\n",
    "                if iflow == 'nwm': #already in the correct format\n",
    "                    continue\n",
    "                if iflow == 'sac': #already in the correct format\n",
    "                    flow_mm[iflow] = xr.DataArray(np.array(flow_mm[iflow].values, dtype='float32'), \n",
    "                                   coords=dict(datetime=flow_mm[iflow].index.values), dims=['datetime'])\n",
    "                else:\n",
    "                    flow_mm[iflow] = xr.DataArray(flow_mm[iflow].values[:,0], \n",
    "                                   coords=dict(datetime=flow_mm[iflow].index.values), dims=['datetime'])\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            calculate_all_metrics_for_frequency_analysis(analysis_dict_temp_small, flow_mm)\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            #Now that the basin has been analyzed successfully, add it to the larger dictionary\n",
    "            analysis_dict_temp_large[basin_0str] = analysis_dict_temp_small\n",
    "            #------------------------------------------------------------------------------------------------- \n",
    "\n",
    "        st1_dict[forcing_type] = {metric:{model:[] for model in models} for metric in loop_these_metrics}\n",
    "        for ib, basin_0str in enumerate(basin_list):\n",
    "            try:\n",
    "                for metric in loop_these_metrics:\n",
    "                    for model in models:\n",
    "                        st1_dict[forcing_type][metric][model].extend(analysis_dict_temp_large[basin_0str][metric][model])\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    with open(\"statistics_table_st1.pkl\", 'wb') as fb:\n",
    "        pkl.dump(st1_dict, fb)\n",
    "        \n",
    "    with open(\"st1_flows_stacked.pkl\", 'wb') as fb:\n",
    "        pkl.dump(st1_dict_stackz, fb)\n",
    "        \n",
    "else:\n",
    "        \n",
    "    with open('statistics_table_st1.pkl', 'rb') as fb:\n",
    "        st1_dict = pkl.load(fb)\n",
    "        \n",
    "    with open('st1_flows_stacked.pkl', 'rb') as fb:\n",
    "        st1_dict_stackz = pkl.load(fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual information LSTM 0.39\n",
      "Mutual information MC 0.37\n",
      "Mutual information SAC-SMA 0.34\n",
      "Mutual information LSTM 0.4\n",
      "Mutual information MC 0.37\n",
      "Mutual information SAC-SMA 0.33\n"
     ]
    }
   ],
   "source": [
    "# Included in the text, but not in a table or figure.\n",
    "for forcing_type in ['nldas', 'daymet']:\n",
    "    mi_lstm = calc_MI(np.array(st1_dict_stackz[forcing_type]['obs'])[:,0], \n",
    "                      np.array(st1_dict_stackz[forcing_type]['lstm'])[:,0], 100)\n",
    "    mi_mc = calc_MI(np.array(st1_dict_stackz[forcing_type]['obs'])[:,0], \n",
    "                    np.array(st1_dict_stackz[forcing_type]['mc'])[:,0], 100)\n",
    "    mi_sac = calc_MI(np.array(st1_dict_stackz[forcing_type]['obs'])[:,0], \n",
    "                     np.array(st1_dict_stackz[forcing_type]['sac']), 100)\n",
    "    print(\"Mutual information LSTM\", np.round(mi_lstm,2))\n",
    "    print(\"Mutual information MC\", np.round(mi_mc,2))\n",
    "    print(\"Mutual information SAC-SMA\", np.round(mi_sac,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE  &0.77 $\\pm$ -0.02 &0.76 $\\pm$ -0.01 &0.65 $\\pm$ -0.03 &0.74 $\\pm$ -0.01 &0.74 $\\pm$ -0.01 &0.67 $\\pm-0.02$ \\\n",
      "KGE  &0.76 $\\pm$ -0.02 &0.76 $\\pm$ -0.02 &0.59 $\\pm$ n/a &0.74 $\\pm$ -0.02 &0.74 $\\pm$ -0.02 &0.68 $\\pm-0.02$ \\\n",
      "Pearson-r  &0.89 $\\pm$ -0.01 &0.88 $\\pm$ -0.01 &0.83 $\\pm$ n/a &0.88 $\\pm$ -0.01 &0.87 $\\pm$ -0.01 &0.83 $\\pm-0.01$ \\\n",
      "Alpha-NSE  &0.85 $\\pm$ -0.01 &0.84 $\\pm$ -0.01 &0.76 $\\pm$ -0.02 &0.81 $\\pm$ -0.02 &0.81 $\\pm$ -0.02 &0.78 $\\pm-0.02$ \\\n",
      "Beta-NSE  &-0.04 $\\pm$ -0.01 &-0.03 $\\pm$ -0.01 &0.06 $\\pm$ -0.01 &-0.03 $\\pm$ -0.01 &-0.02 $\\pm$ -0.01 &-0.01 $\\pm-0.01$ \\\n",
      "Peak-Timing &0.3 $\\pm-0.03$ &0.3 $\\pm-0.03$ &0.38 $\\pm-0.06$ &0.32 $\\pm-0.03$ &0.31 $\\pm$-0.03 &0.41 $\\pm$ -0.06\\\n"
     ]
    }
   ],
   "source": [
    "round_to = 2\n",
    "for metric in table_metrics:\n",
    "    \n",
    "    printstuf = {forcing_type:{model:{} for model in ['lstm', 'mc','sac']} for forcing_type in ['daymet','nldas']}\n",
    "\n",
    "    for forcing_type in ['daymet','nldas']:\n",
    "        for model in ['lstm', 'mc','sac']:\n",
    "            data=st1_dict[forcing_type][metric][model]\n",
    "            printstuf[forcing_type][model]['mean']=np.round(np.nanmedian(data),round_to)\n",
    "            if np.abs(st.t.interval(alpha=0.95, df=len(data)-1,loc=0,scale=st.sem(data))[0]) < \\\n",
    "                np.abs(printstuf[forcing_type][model]['mean']):\n",
    "                conf = np.round(st.t.interval(alpha=0.95, df=len(data)-1, loc=0,scale=st.sem(data))[0],round_to)\n",
    "            else:\n",
    "                conf = 'n/a'\n",
    "            printstuf[forcing_type][model]['conf']=conf\n",
    "    \n",
    "    \n",
    "    print('{}  &{} $\\pm$ {} &{} $\\pm$ {} &{} $\\pm$ {} &{} $\\pm$ {} &{} $\\pm$ {} &{} $\\pm{}$ \\\\'.format(metric,\n",
    "                      printstuf['daymet']['lstm']['mean'],printstuf['daymet']['lstm']['conf'],\n",
    "                                                                               \n",
    "                      printstuf['daymet']['mc']['mean'],printstuf['daymet']['mc']['conf'],\n",
    "                                                                               \n",
    "                      printstuf['daymet']['sac']['mean'],printstuf['daymet']['sac']['conf'],\n",
    "                                                \n",
    "                      printstuf['nldas']['lstm']['mean'],printstuf['nldas']['lstm']['conf'],\n",
    "                      \n",
    "                      printstuf['nldas']['mc']['mean'],printstuf['nldas']['mc']['conf'],\n",
    "                                                                               \n",
    "                      printstuf['nldas']['sac']['mean'],printstuf['nldas']['sac']['conf']))\n",
    "                                            \n",
    "printstuf = {forcing_type:{model:{} for model in ['lstm', 'mc','sac']} for forcing_type in ['daymet','nldas']}\n",
    "for forcing_type in ['daymet','nldas']:\n",
    "    for model in ['lstm', 'mc','sac']:\n",
    "        data=st1_dict[forcing_type]['Peak-Timing'][model]\n",
    "        printstuf[forcing_type][model]['mean']=np.round(np.nanmedian(data),round_to)\n",
    "        printstuf[forcing_type][model]['conf']=np.round(st.t.interval(alpha=0.95, \n",
    "                                                             df=len(data)-1,\n",
    "                                                             loc=0,scale=st.sem(data))[0],round_to)\n",
    "print('Peak-Timing &{} $\\pm{}$ &{} $\\pm{}$ &{} $\\pm{}$ &{} $\\pm{}$ &{} $\\pm${} &{} $\\pm$ {}\\\\'.format(\n",
    "                      printstuf['daymet']['lstm']['mean'],printstuf['daymet']['lstm']['conf'],\n",
    "                                                                               \n",
    "                      printstuf['daymet']['mc']['mean'],printstuf['daymet']['mc']['conf'],\n",
    "                                                                               \n",
    "                      printstuf['daymet']['sac']['mean'],printstuf['daymet']['sac']['conf'],\n",
    "                                                \n",
    "                      printstuf['nldas']['lstm']['mean'],printstuf['nldas']['lstm']['conf'],\n",
    "                      \n",
    "                      printstuf['nldas']['mc']['mean'],printstuf['nldas']['mc']['conf'],\n",
    "                                                                               \n",
    "                      printstuf['nldas']['sac']['mean'],printstuf['nldas']['sac']['conf']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------\n",
    "# NWM Time Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second test period (1995-2014) allows us to benchmark against the NWM-Rv2,  which does not provide data prior to2851995. Most of these scores are broadly  equivalent to the metrics for the same models reported for the test period 1989-1999, with the exception of the FHV (high flow bias), FLV (low flow bias), add FMS (flow duration curve bias).  These metrics dependheavily on the observed flow characteristics during a particular test period and,  because they are less stable, are somewhat lessuseful in terms of drawing general conclusions.  We report them here primarily for continuity with previous studies (Kratzertet al., 2019c, b, 2020;  Frame et al., 2020; Nearing et al., 2020b; Klotz et al., 2021; Gauch et al., 2021a), and one of the  objectives of this paper (Section 2.4 is to expand on the high flow (FHV) analysis specifically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NWM Time Split\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------\n",
    "# Set up lists to use in loops\n",
    "loop_these_metrics = metrics.get_available_metrics()\n",
    "models =        ['nwm', 'lstm','mc', 'sac']\n",
    "flows =         ['nwm', 'lstm','mc', 'sac', 'obs']\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "calculate_metrics_now = False\n",
    "if calculate_metrics_now:\n",
    "    lstm_results_time_split2 = {}\n",
    "    mclstm_results_time_split2 = {}\n",
    "    sacsma_results_time_split2 = {}\n",
    "    for forcing_type in ['nldas', 'daymet']:\n",
    "        with open('./model_output_for_analysis/lstm_time_split2_{}.p'.format(forcing_type), 'rb') as fb:\n",
    "            lstm_results_time_split2[forcing_type] = pkl.load(fb)\n",
    "        with open('./model_output_for_analysis/mclstm_time_split2_{}.p'.format(forcing_type), 'rb') as fb:\n",
    "            mclstm_results_time_split2[forcing_type] = pkl.load(fb)\n",
    "        with open('./model_output_for_analysis/sacsma_time_split2_{}.p'.format(forcing_type), 'rb') as fb:\n",
    "            sacsma_results_time_split2[forcing_type] = pkl.load(fb)\n",
    "\n",
    "    with open('./model_output_for_analysis/nwm_chrt_v2_1d_local.p', 'rb') as fb:\n",
    "        nwm_results = pkl.load(fb)\n",
    "\n",
    "    basin_list = sacsma_results_time_split2[forcing_type].keys()\n",
    "\n",
    "    st2_dict = {}\n",
    "\n",
    "    for forcing_type in ['nldas', 'daymet']:\n",
    "\n",
    "        analysis_dict_temp_large = {}\n",
    "        #-------------------------------------------------------------------------------------------------            \n",
    "\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "        #-----LOOP THROUGH BASINS------------------------------------------------------------------------\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "        for ib, basin_0str in enumerate(basin_list): \n",
    "            basin_int = int(basin_0str)\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Get the NWM data for this basin in an xarray dataset.\n",
    "            xr_nwm = xr.DataArray(nwm_results[basin_0str]['streamflow'].values, \n",
    "                     coords=[nwm_results[basin_0str]['streamflow'].index], \n",
    "                     dims=['datetime'])\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Setting up the dictionary for the single basin results. Then will add to the overall dict.\n",
    "            analysis_dict_temp_small = {metric:{model:[] for model in models} for metric in loop_these_metrics}\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # We need the basin area to convert to CFS, to interpolate the RI from LPIII\n",
    "            basin_area = pd_attributes.loc[basin_int, 'area_geospa_fabric']\n",
    "            basin_str = str(basin_int).zfill(8)\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            #----  Set the time period for metrics   ---------------------------------------------------------\n",
    "            date_from = '1996-10'\n",
    "            date_to = '2014-09'\n",
    "            #-------------------------------------------------------------------------------------------------    \n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Make dictionary with all the flows\n",
    "            flow_mm = {}\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------        \n",
    "             # NWM data\n",
    "            sim_nwm = xr_nwm.loc[date_from:date_to]\n",
    "            # convert from CFS to mm/day\n",
    "            # fm3/s * 3600 sec/hour * 24 hour/day / (m2 * mm/m)\n",
    "            flow_mm['nwm'] = sim_nwm*3600*24/(basin_area*1000)\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Standard LSTM data trained on all years\n",
    "            xrr = lstm_results_time_split2[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_sim']\n",
    "            flow_mm['lstm'] = pd.DataFrame(data=xrr.values,index=xrr.date.values).loc[date_from:date_to]\n",
    "            #-------------------------------------------------------------------------------------------------        \n",
    "            # Mass Conserving LSTM data\n",
    "            xrr = mclstm_results_time_split2[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_sim']\n",
    "            flow_mm['mc'] = pd.DataFrame(data=xrr.values,index=xrr.date.values).loc[date_from:date_to]\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # SACSMA Sinlge run\n",
    "            df = sacsma_results_time_split2[forcing_type][basin_0str]\n",
    "            flow_mm['sac'] = df.loc[date_from:date_to]\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # OBSERVATIONS\n",
    "            xrr = mclstm_results_time_split2[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_obs']\n",
    "            flow_mm['obs'] = pd.DataFrame(data=xrr.values,index=xrr.date.values).loc[date_from:date_to]\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Make all xarray data similar\n",
    "            for iflow in flows:\n",
    "                if iflow == 'nwm': #already in the correct format\n",
    "                    continue\n",
    "                if iflow == 'sac': #already in the correct format\n",
    "                    flow_mm[iflow] = xr.DataArray(np.array(flow_mm[iflow].values, dtype='float32'), \n",
    "                                   coords=dict(datetime=flow_mm[iflow].index.values), dims=['datetime'])\n",
    "                else:\n",
    "                    flow_mm[iflow] = xr.DataArray(flow_mm[iflow].values[:,0], \n",
    "                                   coords=dict(datetime=flow_mm[iflow].index.values), dims=['datetime'])\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            calculate_all_metrics_for_frequency_analysis(analysis_dict_temp_small, flow_mm)\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            #Now that the basin has been analyzed successfully, add it to the larger dictionary\n",
    "            analysis_dict_temp_large[basin_0str] = analysis_dict_temp_small\n",
    "            #------------------------------------------------------------------------------------------------- \n",
    "\n",
    "        st2_dict[forcing_type] = {metric:{model:[] for model in models} for metric in loop_these_metrics}\n",
    "        for ib, basin_0str in enumerate(basin_list):\n",
    "            try:\n",
    "                for metric in loop_these_metrics:\n",
    "                    for model in models:\n",
    "                        st2_dict[forcing_type][metric][model].extend(analysis_dict_temp_large[basin_0str][metric][model])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    with open(\"statistics_table_st2.pkl\", 'wb') as fb:\n",
    "        pkl.dump(st2_dict, fb)\n",
    "        \n",
    "else:\n",
    "    \n",
    "    with open('statistics_table_st2.pkl', 'rb') as fb:\n",
    "        st2_dict = pkl.load(fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE  &0.74 $\\pm$ -0.02 &0.74 $\\pm$ -0.02 &0.59 $\\pm$ -0.08 &0.71 $\\pm$ -0.05 &0.72 $\\pm$ -0.02 &0.63 $\\pm$ -0.05 &0.63 $\\pm$ -0.05 \\\n",
      "KGE  &0.78 $\\pm$ -0.02 &0.77 $\\pm$ -0.02 &0.56 $\\pm$ n/a &0.77 $\\pm$ -0.02 &0.74 $\\pm$ -0.02 &0.68 $\\pm$ -0.02 &0.67 $\\pm$ -0.05 \\\n",
      "Pearson-r  &0.88 $\\pm$ -0.01 &0.88 $\\pm$ -0.01 &0.81 $\\pm$ n/a &0.86 $\\pm$ -0.01 &0.86 $\\pm$ -0.01 &0.81 $\\pm$ -0.01 &0.82 $\\pm$ -0.01 \\\n",
      "Alpha-NSE  &0.96 $\\pm$ -0.02 &0.91 $\\pm$ -0.01 &0.88 $\\pm$ -0.02 &0.94 $\\pm$ -0.02 &0.87 $\\pm$ -0.02 &0.83 $\\pm$ -0.02 &0.85 $\\pm$ -0.03 \\\n",
      "Beta-NSE  &0.03 $\\pm$ -0.01 &0.03 $\\pm$ -0.01 &0.13 $\\pm$ -0.02 &0.01 $\\pm$ -0.01 &-0.01 $\\pm$ -0.01 &-0.01 $\\pm$ n/a &-0.01 $\\pm$ n/a \\\n",
      "Peak-Timing &0.34 $\\pm-0.03$ &0.33 $\\pm-0.03$ &0.45 $\\pm-0.06$ &0.38 $\\pm$ -0.03 &0.4 $\\pm$-0.03 &0.53 $\\pm$ -0.06 &0.54 $\\pm$ -0.05\\\n"
     ]
    }
   ],
   "source": [
    "table_metrics = ['NSE','KGE','Pearson-r','Alpha-NSE','Beta-NSE']#,'FHV','FLV','FMS']\n",
    "round_to = 2\n",
    "for metric in table_metrics:\n",
    "    \n",
    "    printstuf = {forcing_type:{model:{} for model in ['lstm', 'mc','sac', 'nwm']} for forcing_type in ['daymet','nldas']}\n",
    "\n",
    "    for forcing_type in ['daymet','nldas']:\n",
    "        if forcing_type == 'daymet':\n",
    "            modelz = ['lstm', 'mc','sac']\n",
    "        if forcing_type == 'nldas':\n",
    "            modelz = ['lstm', 'mc','sac', 'nwm']\n",
    "        for model in modelz:\n",
    "            data=st2_dict[forcing_type][metric][model]\n",
    "            printstuf[forcing_type][model]['mean']=np.round(np.nanmedian(data),round_to)\n",
    "            if np.abs(st.t.interval(alpha=0.95, df=len(data)-1,loc=0,scale=st.sem(data))[0]) < \\\n",
    "                np.abs(printstuf[forcing_type][model]['mean']):\n",
    "                conf = np.round(st.t.interval(alpha=0.95, df=len(data)-1, loc=0,scale=st.sem(data))[0],round_to)\n",
    "            else:\n",
    "                conf = 'n/a'\n",
    "            printstuf[forcing_type][model]['conf']=conf\n",
    "    \n",
    "    \n",
    "    print('{}  &{} $\\pm$ {} &{} $\\pm$ {} &{} $\\pm$ {} &{} $\\pm$ {} &{} $\\pm$ {} &{} $\\pm$ {} &{} $\\pm$ {} \\\\'.format(metric,\n",
    "                      printstuf['daymet']['lstm']['mean'],printstuf['daymet']['lstm']['conf'],\n",
    "                                                                               \n",
    "                      printstuf['daymet']['mc']['mean'],printstuf['daymet']['mc']['conf'],\n",
    "                                                                               \n",
    "                      printstuf['daymet']['sac']['mean'],printstuf['daymet']['sac']['conf'],\n",
    "                                                \n",
    "                      printstuf['nldas']['lstm']['mean'],printstuf['nldas']['lstm']['conf'],\n",
    "                      \n",
    "                      printstuf['nldas']['mc']['mean'],printstuf['nldas']['mc']['conf'],\n",
    "                                                                               \n",
    "                      printstuf['nldas']['sac']['mean'],printstuf['nldas']['sac']['conf'],\n",
    "                    \n",
    "                      printstuf['nldas']['nwm']['mean'],printstuf['nldas']['nwm']['conf']))\n",
    "                                            \n",
    "printstuf = {forcing_type:{model:{} for model in ['lstm', 'mc','sac', 'nwm']} for forcing_type in ['daymet','nldas']}\n",
    "for forcing_type in ['daymet','nldas']:\n",
    "    if forcing_type == 'daymet':\n",
    "        modelz = ['lstm', 'mc','sac']\n",
    "    if forcing_type == 'nldas':\n",
    "        modelz = ['lstm', 'mc','sac', 'nwm']\n",
    "    for model in modelz:\n",
    "        data=st2_dict[forcing_type]['Peak-Timing'][model]\n",
    "        printstuf[forcing_type][model]['mean']=np.round(np.nanmedian(data),round_to)\n",
    "        printstuf[forcing_type][model]['conf']=np.round(st.t.interval(alpha=0.95, \n",
    "                                                             df=len(data)-1,\n",
    "                                                             loc=0,scale=st.sem(data))[0],round_to)\n",
    "print('Peak-Timing &{} $\\pm{}$ &{} $\\pm{}$ &{} $\\pm{}$ &{} $\\pm$ {} &{} $\\pm${} &{} $\\pm$ {} &{} $\\pm$ {}\\\\'.format(\n",
    "                      printstuf['daymet']['lstm']['mean'],printstuf['daymet']['lstm']['conf'],\n",
    "                                                                               \n",
    "                      printstuf['daymet']['mc']['mean'],printstuf['daymet']['mc']['conf'],\n",
    "                                                                               \n",
    "                      printstuf['daymet']['sac']['mean'],printstuf['daymet']['sac']['conf'],\n",
    "                                                \n",
    "                      printstuf['nldas']['lstm']['mean'],printstuf['nldas']['lstm']['conf'],\n",
    "                      \n",
    "                      printstuf['nldas']['mc']['mean'],printstuf['nldas']['mc']['conf'],\n",
    "                                                                               \n",
    "                      printstuf['nldas']['sac']['mean'],printstuf['nldas']['sac']['conf'],\n",
    "\n",
    "                      printstuf['nldas']['nwm']['mean'],printstuf['nldas']['nwm']['conf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

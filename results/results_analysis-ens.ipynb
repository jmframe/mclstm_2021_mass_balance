{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import xarray as xr\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import metrics\n",
    "\n",
    "import random\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./model_output_for_analysis/nwm_chrt_v2_1d_local.p', 'rb') as fb:\n",
    "    nwm_results = pkl.load(fb)\n",
    "\n",
    "lstm_results_time_split1={}\n",
    "mclstm_results_time_split1={}\n",
    "sacsma_results_time_split1={}\n",
    "lstm_results_time_split2={}\n",
    "mclstm_results_time_split2={}\n",
    "sacsma_results_time_split2={}\n",
    "\n",
    "for forcing_type in ['nldas', 'daymet']:\n",
    "    \n",
    "    with open('./model_output_for_analysis/lstm_time_split1_{}_ens.p'.format(forcing_type), 'rb') as fb:\n",
    "        lstm_results_time_split1[forcing_type] = pkl.load(fb)\n",
    "    with open('./model_output_for_analysis/mclstm_time_split1_{}_ens.p'.format(forcing_type), 'rb') as fb:\n",
    "        mclstm_results_time_split1[forcing_type] = pkl.load(fb)\n",
    "    with open('./model_output_for_analysis/sacsma_time_split1_{}_ens.p'.format(forcing_type), 'rb') as fb:\n",
    "        sacsma_results_time_split1[forcing_type] = pkl.load(fb)\n",
    "\n",
    "    with open('./model_output_for_analysis/lstm_time_split2_{}.p'.format(forcing_type), 'rb') as fb:\n",
    "        lstm_results_time_split2[forcing_type] = pkl.load(fb)\n",
    "    with open('./model_output_for_analysis/mclstm_time_split2_{}.p'.format(forcing_type), 'rb') as fb:\n",
    "        mclstm_results_time_split2[forcing_type] = pkl.load(fb)\n",
    "    with open('./model_output_for_analysis/sacsma_time_split2_{}.p'.format(forcing_type), 'rb') as fb:\n",
    "        sacsma_results_time_split2[forcing_type] = pkl.load(fb)\n",
    "\n",
    "train_split_type_model_set = {'time_split1':{'nwm':nwm_results, \n",
    "                                           'lstm':lstm_results_time_split1,\n",
    "                                            'mc':mclstm_results_time_split1,\n",
    "                                            'sac':sacsma_results_time_split1},\n",
    "                              'time_split2':{'nwm':nwm_results, \n",
    "                                           'lstm':lstm_results_time_split2,\n",
    "                                            'mc':mclstm_results_time_split2,\n",
    "                                            'sac':sacsma_results_time_split2}}\n",
    "\n",
    "range_for_analysis = {'time_split1': [1989,1999],'time_split2': [1996, 2014]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert flow to   CFS mm -> ft     km^2 -> ft^2    hr->s\n",
    "conversion_factor = 0.00328084 * 10763910.41671 / 3600 / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the basins in the analysis\n",
    "#basin_list = list(lstm_results_time_split.keys())\n",
    "\n",
    "# Camels attributes with RI information\n",
    "dataName = '../data/camels_attributes.csv'\n",
    "# load the data with pandas\n",
    "pd_attributes = pd.read_csv(dataName, sep=',', index_col='gauge_id')\n",
    "\n",
    "# Add the basin ID as a 8 element string with a leading zero if neccessary\n",
    "basin_id_str = []\n",
    "for a in pd_attributes.index.values:\n",
    "    basin_id_str.append(str(a).zfill(8))\n",
    "pd_attributes['basin_id_str'] = basin_id_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all the SACSMA runs and check that the results are good. \n",
    "# Get a list of basins that has good calibration results.\n",
    "\n",
    "basin_list_all_camels = list(pd_attributes['basin_id_str'].values)\n",
    "basin_list_sacsma_good = {ts:copy.deepcopy(basin_list_all_camels) for ts in ['time_split1', 'time_split2']}\n",
    "\n",
    "for ib, basin_0str in enumerate(basin_list_all_camels): \n",
    "    remove_basin_id_from_list = False\n",
    "    for train_split_type in ['time_split1', 'time_split2']:\n",
    "        for forcing_type in ['nldas', 'daymet']:\n",
    "            \n",
    "            if basin_0str not in list(train_split_type_model_set[train_split_type]['sac'][forcing_type].columns):\n",
    "                remove_basin_id_from_list = True\n",
    "            elif train_split_type_model_set[train_split_type]['sac'][forcing_type][basin_0str].sum() <=0:\n",
    "                remove_basin_id_from_list = True\n",
    "                \n",
    "            if train_split_type == 'time_split2' and forcing_type == 'nldas':\n",
    "                if basin_0str not in list(train_split_type_model_set[train_split_type]['nwm'].keys()):\n",
    "                    remove_basin_id_from_list = True\n",
    "                \n",
    "    if remove_basin_id_from_list:\n",
    "        basin_list_sacsma_good[train_split_type].remove(basin_0str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------\n",
    "# Solve this problem. I think it is the xarray structures...\n",
    "# isibleDeprecationWarning: Creating an ndarray from ragged nested sequences \n",
    "# (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. \n",
    "# If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_metrics_for_frequency_analysis(analysis_dict, flows, recurrance_interval):\n",
    "\n",
    "    sims = list(flows.keys())[:-1]\n",
    "\n",
    "    for metric in metrics.get_available_metrics():\n",
    "\n",
    "        score = {sim:0 for sim in sims}\n",
    "    \n",
    "        analysis_dict[metric]['ri'].append(recurrance_interval)\n",
    "    \n",
    "        if metric == 'NSE':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.nse(flows['obs'],flows[sim])\n",
    "        if metric == 'MSE':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.mse(flows['obs'],flows[sim])\n",
    "        if metric == 'RMSE':\n",
    "            for sim in sims:\n",
    "                 score[sim] = metrics.rmse(flows['obs'],flows[sim])\n",
    "        if metric == 'KGE':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.kge(flows['obs'],flows[sim])\n",
    "        if metric == 'Alpha-NSE':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.alpha_nse(flows['obs'],flows[sim])\n",
    "        if metric == 'Beta-NSE':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.beta_nse(flows['obs'],flows[sim])\n",
    "        if metric == 'Pearson-r':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.pearsonr(flows['obs'],flows[sim])\n",
    "        if metric == 'Peak-Timing':\n",
    "            for sim in sims:\n",
    "                score[sim] = np.abs(metrics.mean_peak_timing(flows['obs'],flows[sim]))\n",
    "        if metric == 'FHV':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.fdc_fhv(flows['obs'],flows[sim])\n",
    "        if metric == 'FLV':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.fdc_flv(flows['obs'],flows[sim])\n",
    "        if metric == 'FMS':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.fdc_fms(flows['obs'],flows[sim])\n",
    "\n",
    "        for sim in sims:\n",
    "            analysis_dict[metric][sim].append(score[sim])\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing  nldas\n",
      "Analyzing  daymet\n"
     ]
    }
   ],
   "source": [
    "flows = ['nwm', 'lstm', 'mc', 'sac', 'obs']\n",
    "for forcing_type in ['nldas', 'daymet']:\n",
    "    print('Analyzing ',forcing_type)\n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    analysis_dict_names = {'time_split1':'frequency_analysis_dict_time_split1_{}.pkl'.format(forcing_type),\n",
    "                           'time_split2':'frequency_analysis_dict_time_split2_{}.pkl'.format(forcing_type)}\n",
    "    peak_flows_dict_names = {'time_split1':'peak_annual_flows_dict_time_split1_{}.pkl'.format(forcing_type),\n",
    "                             'time_split2':'peak_annual_flows_dict_time_split2_{}.pkl'.format(forcing_type)}\n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    #----------   If the calcs have been done, then just read them in.\n",
    "    if True:\n",
    "        with open(analysis_dict_names[train_split_type], 'rb') as fb:\n",
    "            analysis_dict_all = pkl.load(fb)\n",
    "        with open(peak_flows_dict_names[train_split_type], 'rb') as fb:\n",
    "            peak_flows_dict = pkl.load(fb)\n",
    "            \n",
    "    else:\n",
    "\n",
    "        for train_split_type in ['time_split1', 'time_split2']:\n",
    "            print('    Analyzing ',train_split_type)\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Set up lists\n",
    "            if train_split_type == 'time_split1':\n",
    "                models = ['lstm', 'mc', 'sac']\n",
    "                flows = ['lstm', 'mc', 'sac', 'obs']\n",
    "            else:\n",
    "                models = ['nwm', 'lstm', 'mc', 'sac']\n",
    "                flows = ['nwm', 'lstm', 'mc', 'sac', 'obs']\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Place the data here\n",
    "            analysis_dict_all = {}\n",
    "            peak_flows_dict = {i:[] for i in models_obs_ri}\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            #-----LOOP THROUGH BASINS------------------------------------------------------------------------\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "            for ib, basin_0str in enumerate(basin_list): \n",
    "                basin_int = int(basin_0str)\n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                # Get the NWM data for this basin in an xarray dataset.\n",
    "                xr_nwm = xr.DataArray(train_split_type_model_set[train_split_type]['nwm'][basin_0str]['streamflow'].values, \n",
    "                         coords=[nwm_results[basin_0str]['streamflow'].index], \n",
    "                         dims=['datetime'])\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                # Setting up the dictionary for the single basin results. Then will add to the overall dict.\n",
    "                analysis_dict = {metric:{model:[] for model in models_ri} for metric in metrics.get_available_metrics()}\n",
    "                extra_metrics = ['beta-abs', 'peakQ', 'peakRI', 'peakT', 'peakQ-perc', 'peakRI-perc', 'peakT-abs']\n",
    "                for extra_metric in extra_metrics:\n",
    "                    analysis_dict[extra_metric] = {model:[] for model in models_ri}\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                # We need the basin area to convert to CFS, to interpolate the RI from LPIII\n",
    "                basin_area = pd_attributes.loc[basin_int, 'area_geospa_fabric']\n",
    "                basin_str = tools.gauge_id_str(basin_int)\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                # Get the log pearson III results\n",
    "                b17 = tools.read_b17(basin_str)\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                # Get the peak flows, but then cut them to just the validation year.\n",
    "                peakflows = tools.read_peak_flows(basin_str)\n",
    "                peakflows['wateryear'] = [int(tools.get_water_year(int(peakflows.iloc[i,0].split('-')[0]), \n",
    "                                          int(peakflows.iloc[i,0].split('-')[1]))) for i in range(peakflows.shape[0])]\n",
    "                peakflows = pd.DataFrame(peakflows.set_index('wateryear'))\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                #----  LOOP THROUGH THE WATER YEARS   ------------------------------------------------------------\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                for water_year in range(range_for_analysis[train_split_type][0], \n",
    "                                        range_for_analysis[train_split_type][1]):\n",
    "                    date_from = str(water_year-1)+'-10'\n",
    "                    date_to = str(water_year)+'-09'\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Figure out what the actual recurrence interval is for the basin-year. \n",
    "                    # We'll use this to categorize the basin-year, but then calc the metrics with the observations.\n",
    "                    if water_year not in list(peakflows.index.values):\n",
    "                        #print(\"water year not in record\")\n",
    "                        continue\n",
    "                    peak_date = peakflows.loc[water_year, 0]\n",
    "                    if isinstance(peakflows.loc[water_year, 1], str):\n",
    "                        peak_flow = float(peakflows.loc[water_year, 1].replace(\" \", \"\"))\n",
    "                    else:\n",
    "                        peak_flow = peakflows.loc[water_year, 1]\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Make dictionary with all the flows\n",
    "                    flow_mm = {}\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------        \n",
    "                     # NWM data\n",
    "                    if train_split_type != 'time_split1':\n",
    "                        sim_nwm = xr_nwm.loc[date_from:date_to]\n",
    "                        # convert from CFS to mm/day\n",
    "                        # fm3/s * 3600 sec/hour * 24 hour/day / (m2 * mm/m)\n",
    "                        flow_mm['nwm'] = sim_nwm*3600*24/(basin_area*1000)\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Standard LSTM \n",
    "                    xrr = train_split_type_model_set[train_split_type]['lstm'][forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_sim']\n",
    "                    flow_mm['lstm'] = pd.DataFrame(data=xrr.values,index=xrr.date.values).loc[date_from:date_to]\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Mass-conserving LSTM data trained on all years\n",
    "                    xrr = train_split_type_model_set[train_split_type]['mc'][forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_sim']\n",
    "                    flow_mm['mc'] = pd.DataFrame(data=xrr.values,index=xrr.date.values).loc[date_from:date_to]\n",
    "                    #-------------------------------------------------------------------------------------------------        \n",
    "                    # SACSMA \n",
    "                    df = train_split_type_model_set[train_split_type]['sac'][forcing_type][basin_0str]\n",
    "                    flow_mm['sac'] = df.loc[date_from:date_to]\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # OBSERVATIONS\n",
    "                    xrr = train_split_type_model_set[train_split_type]['mc'][forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_obs']\n",
    "                    flow_mm['obs'] = pd.DataFrame(data=xrr.values,index=xrr.date.values).loc[date_from:date_to]\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "                    #------------------------------------------------------------------------------------------------- \n",
    "                    for iflow in models:\n",
    "                        analysis_dict['beta-abs'][iflow].append(np.abs(analysis_dict['Beta-NSE'][iflow][-1]))\n",
    "                    #-------------------------------------------------------------------------------------------------        \n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                #Now that the basin has been analyzed successfully, add it to the larger dictionary\n",
    "                analysis_dict_all[basin_0str] = analysis_dict\n",
    "                #------------------------------------------------------------------------------------------------- \n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            with open(analysis_dict_names[train_split_type], 'wb') as fb:\n",
    "                pkl.dump(analysis_dict_all, fb)\n",
    "            with open(peak_flows_dict_names[train_split_type], 'wb') as fb:\n",
    "                pkl.dump(peak_flows_dict, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = ['lstm', 'mc', 'sac', 'obs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tsplt time_split2\n",
      "forcing_type  nldas\n",
      "forcing_type  daymet\n",
      "flows ['lstm', 'mc', 'sac', 'obs']\n",
      "tsplt time_split1\n",
      "forcing_type  nldas\n",
      "flows ['lstm', 'mc', 'sac', 'obs']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataArray' object has no attribute 'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-54446ee6295d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtsplt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'time_split2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mxrr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_results_time_split2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mforcing_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbasin_0str\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1D'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'QObs(mm/d)_sim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mflow_mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lstm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxrr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxrr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0;31m#-------------------------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Mass-conserving LSTM data trained on all years\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programs/anaconda3/envs/mc-paper/lib/python3.7/site-packages/xarray/core/common.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    227\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         raise AttributeError(\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0;34m\"{!r} object has no attribute {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataArray' object has no attribute 'date'"
     ]
    }
   ],
   "source": [
    "forcing_products = ['nldas','daymet']\n",
    "\n",
    "file_name_map = {'nldas':'nldas', 'daymet':'cida'}\n",
    "precip_column_map = {'nldas':'PRCP(mm/day)', 'daymet':'prcp(mm/day)'}\n",
    "\n",
    "total_mass_error = {forcing_type:{time_split:{'absolute':{flow:[] for flow in flows}, \n",
    "              'positive':{flow:[] for flow in flows}, \n",
    "              'negative':{flow:[] for flow in flows}} for time_split in ['time_split1', 'time_split2']} for \\\n",
    "               forcing_type in forcing_products}\n",
    "for err_type in ['absolute','positive', 'negative']:\n",
    "    total_mass_error['nldas']['time_split2'][err_type]['nwm']=[]\n",
    "        \n",
    "cumulative_mass_all = {forcing_type:{time_split:{} for time_split in ['time_split1', 'time_split2']} for \\\n",
    "                       forcing_type in forcing_products}\n",
    "total_mass = {forcing_type:{time_split:{} for time_split in ['time_split1', 'time_split2']} for \\\n",
    "                       forcing_type in forcing_products}\n",
    "    \n",
    "mass_basin_list={}\n",
    "    \n",
    "for tsplt in ['time_split2', 'time_split1']:\n",
    "    print('tsplt', tsplt)\n",
    "    for forcing_type in forcing_products:\n",
    "\n",
    "        print('forcing_type ',forcing_type)\n",
    "\n",
    "        mass_basin_list[tsplt] = []\n",
    "\n",
    "        forcing_dir = '/home/NearingLab/data/camels_data/basin_dataset_public_v1p2'+\\\n",
    "            '/basin_mean_forcing/{}_all_basins_in_one_directory/'.format(forcing_type)\n",
    "\n",
    "        if tsplt == 'time_split2' and forcing_type == 'nldas':\n",
    "            start_date = pd.Timestamp('1996-10-01')\n",
    "            end_date = pd.Timestamp('2014-01-01')\n",
    "            labelz={'nwm':'NWM*', 'lstm':'LSTM', 'mc':'MC-LSTM','sac':'SAC-SMA', 'obs':'Observed'}\n",
    "            models = ['nwm', 'lstm', 'mc', 'sac']\n",
    "            flows = ['nwm', 'lstm', 'mc', 'sac', 'obs']\n",
    "            basin_list = list(lstm_results_time_split2[forcing_type].keys())[:-1]\n",
    "        elif tsplt == 'time_split2':\n",
    "            start_date = pd.Timestamp('1996-10-01')\n",
    "            end_date = pd.Timestamp('2014-01-01')\n",
    "            labelz={'lstm':'LSTM', 'mc':'MC-LSTM','sac':'SAC-SMA', 'obs':'Observed'}\n",
    "            models = ['lstm', 'mc', 'sac']\n",
    "            flows = ['lstm', 'mc', 'sac', 'obs']\n",
    "            basin_list = list(lstm_results_time_split2[forcing_type].keys())[:-1]\n",
    "            print('flows',flows)\n",
    "        else:\n",
    "            start_date = pd.Timestamp('1989-10-01')\n",
    "            end_date = pd.Timestamp('1999-09-30')\n",
    "            labelz={'lstm':'LSTM', 'mc':'MC-LSTM','sac':'SAC-SMA', 'obs':'Observed'}\n",
    "            models = ['lstm', 'mc', 'sac']\n",
    "            flows = ['lstm', 'mc', 'sac', 'obs']\n",
    "            print('flows',flows)\n",
    "            basin_list = list(lstm_results_time_split1[forcing_type].keys())[:-1]\n",
    "\n",
    "        first_basin = True\n",
    "\n",
    "        for basin_0str in basin_list:\n",
    "            basin_int = int(basin_0str)\n",
    "#            print(basin_0str)\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Reset the total mass to zero for this basin    \n",
    "            cumulative_mass = {flow:[0] for flow in flows}\n",
    "            cumulative_mass['precip'] = [0]\n",
    "            total_mass[forcing_type][tsplt][basin_0str] = {flow:0 for flow in flows}\n",
    "            imass=1\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # We need the basin area to convert to CFS, to interpolate the RI from LPIII\n",
    "            basin_area = pd_attributes.loc[basin_int, 'area_geospa_fabric']\n",
    "            basin_str = str(basin_int).zfill(8)\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Make dictionary with all the flows\n",
    "            flow_mm = {}    \n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            if tsplt == 'time_split2' and forcing_type == 'nldas':\n",
    "                # Get the NWM data for this basin in an xarray dataset.\n",
    "                xr_nwm = xr.DataArray(nwm_results[basin_0str]['streamflow'].values, \n",
    "                         coords=[nwm_results[basin_0str]['streamflow'].index], \n",
    "                         dims=['datetime'])\n",
    "                # convert from CFS to mm/day\n",
    "                # fm3/s * 3600 sec/hour * 24 hour/day / (m2 * mm/m)\n",
    "                flow_mm['nwm'] = xr_nwm.loc[start_date:end_date]*3600*24/(basin_area*1000)\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Standard LSTM \n",
    "            if tsplt == 'time_split1':\n",
    "                xrr = lstm_results_time_split1[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_sim'].loc[start_date:end_date]\n",
    "            if tsplt == 'time_split2':\n",
    "                xrr = lstm_results_time_split2[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_sim'].loc[start_date:end_date]\n",
    "            flow_mm['lstm'] = pd.DataFrame(data=xrr.values,index=xrr.date.values)\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Mass-conserving LSTM data trained on all years\n",
    "            if tsplt == 'time_split1':\n",
    "                xrr = mclstm_results_time_split1[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_sim'].loc[start_date:end_date]\n",
    "            if tsplt == 'time_split2':\n",
    "                xrr = mclstm_results_time_split2[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_sim'].loc[start_date:end_date]\n",
    "            flow_mm['mc'] = pd.DataFrame(data=xrr.values,index=xrr.date.values)\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # SACSMA Mean\n",
    "            if tsplt == 'time_split1':\n",
    "                df = sacsma_results_time_split1[forcing_type][basin_0str].loc[start_date:end_date]\n",
    "            if tsplt == 'time_split2':\n",
    "                df = sacsma_results_time_split2[forcing_type][basin_0str].loc[start_date:end_date]\n",
    "            flow_mm['sac'] = df\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # OBSERVATIONS\n",
    "            if tsplt == 'time_split1':\n",
    "                xrr = mclstm_results_time_split1[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_obs'].loc[start_date:end_date]\n",
    "            if tsplt == 'time_split2':\n",
    "                xrr = mclstm_results_time_split2[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_obs'].loc[start_date:end_date]\n",
    "            flow_mm['obs'] = pd.DataFrame(data=xrr.values,index=xrr.date.values)\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # FORCING\n",
    "            forcing = pd.read_csv(forcing_dir+basin_0str+'_lump_{}_forcing_leap.txt'.format(file_name_map[forcing_type]), \n",
    "                                  delim_whitespace=True, header=3)\n",
    "            if tsplt == 'time_split1':\n",
    "                forcing = forcing.iloc[3560:7214]\n",
    "            if tsplt == 'time_split2':\n",
    "                forcing = forcing.iloc[6118:]\n",
    "            forcing.index=pd.to_datetime((forcing.Year*10000+forcing.Mnth*100+forcing.Day).apply(str),format='%Y%m%d')\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Make sure we are in a time period that all the flow members have values\n",
    "            # If there is missin observations than we can't compare the mass of the observed with simulaitons\n",
    "            skip_basin_because_missing_obs = False\n",
    "            if tsplt == 'time_split1':\n",
    "                obs_temp = mclstm_results_time_split1[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_obs'].date\n",
    "            if tsplt == 'time_split2':\n",
    "                obs_temp = mclstm_results_time_split2[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_obs'].date\n",
    "                \n",
    "            for d in obs_temp:\n",
    "                if d.values < start_date:\n",
    "                    continue\n",
    "                if d.values > end_date:\n",
    "                    break\n",
    "                if np.isnan(flow_mm['obs'].loc[d.values].values[0]):\n",
    "                    skip_basin_because_missing_obs = True\n",
    "                    break\n",
    "                else:\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Keep track of the cumulative mass and add it to the list\n",
    "                    cumulative_mass['precip'].append(forcing[precip_column_map[forcing_type]].loc[d.values] + \\\n",
    "                                                     cumulative_mass['precip'][imass-1])\n",
    "\n",
    "                    cumulative_mass['obs'].append(flow_mm['obs'].loc[d.values].values[0] + \\\n",
    "                                                  cumulative_mass['obs'][imass-1])\n",
    "\n",
    "                    if tsplt == 'time_split2' and forcing_type == 'nldas':\n",
    "                        cumulative_mass['nwm'].append(flow_mm['nwm'].loc[d.values].values + \\\n",
    "                                                      cumulative_mass['nwm'][imass-1])\n",
    "\n",
    "                    cumulative_mass['lstm'].append(flow_mm['lstm'].loc[d.values].values[0] + \\\n",
    "                                                   cumulative_mass['lstm'][imass-1])\n",
    "\n",
    "                    cumulative_mass['mc'].append(flow_mm['mc'].loc[d.values].values[0] + \\\n",
    "                                                 cumulative_mass['mc'][imass-1])\n",
    "\n",
    "                    cumulative_mass['sac'].append(flow_mm['sac'].loc[d.values] + \\\n",
    "                                                  cumulative_mass['sac'][imass-1])\n",
    "                    imass+=1\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # If there is missin observations than we can't compare the mass of the observed with simulaitons            \n",
    "            if skip_basin_because_missing_obs:\n",
    "    #            print(\"skipping basin {} because of missing observations\".format(basin_0str))\n",
    "                continue\n",
    "            else:\n",
    "                mass_basin_list[tsplt].append(basin_0str)\n",
    "\n",
    "            for flow in flows:\n",
    "                total_mass[forcing_type][tsplt][basin_0str][flow] = np.nansum(flow_mm[flow].loc[start_date:end_date])\n",
    "\n",
    "            for flow in flows:\n",
    "                total_mass_error[forcing_type][tsplt]['absolute'][flow].append( \\\n",
    "                                        np.abs(total_mass[forcing_type][tsplt][basin_0str][flow] - \\\n",
    "                                        total_mass[forcing_type][tsplt][basin_0str]['obs'])/ \\\n",
    "                                        total_mass[forcing_type][tsplt][basin_0str]['obs'])\n",
    "                if (total_mass[forcing_type][tsplt][basin_0str][flow] - total_mass[forcing_type][tsplt][basin_0str]['obs']) > 0:\n",
    "                    total_mass_error[forcing_type][tsplt]['positive'][flow].append((\\\n",
    "                                        total_mass[forcing_type][tsplt][basin_0str][flow] - \\\n",
    "                                        total_mass[forcing_type][tsplt][basin_0str]['obs'])/ \\\n",
    "                                        total_mass[forcing_type][tsplt][basin_0str]['obs'])\n",
    "                    total_mass_error[forcing_type][tsplt]['negative'][flow].append(0)\n",
    "                else:\n",
    "                    total_mass_error[forcing_type][tsplt]['negative'][flow].append(( \\\n",
    "                                        total_mass[forcing_type][tsplt][basin_0str][flow] - \\\n",
    "                                        total_mass[forcing_type][tsplt][basin_0str]['obs']) / \\\n",
    "                                        total_mass[forcing_type][tsplt][basin_0str]['obs'])\n",
    "                    total_mass_error[forcing_type][tsplt]['positive'][flow].append(0)\n",
    "\n",
    "            # _______________________________________________________________________\n",
    "            # Keep track of all the cumulative mass through time for each basin\n",
    "            if first_basin and not skip_basin_because_missing_obs:\n",
    "                for flow in flows:\n",
    "                    cumulative_mass_all[forcing_type][tsplt][flow] = np.array(cumulative_mass[flow])\n",
    "                cumulative_mass_all[forcing_type][tsplt]['precip'] = np.array(cumulative_mass['precip'])\n",
    "                first_basin = False\n",
    "            if  not skip_basin_because_missing_obs and not first_basin:\n",
    "                for flow in flows:\n",
    "                    cumulative_mass_all[forcing_type][tsplt][flow] += np.array(cumulative_mass[flow])\n",
    "                cumulative_mass_all[forcing_type][tsplt]['precip'] +=np.array(cumulative_mass['precip'])\n",
    "\n",
    "# _______________________________________________________________________\n",
    "# Save the mass balance results.\n",
    "with open('total_mass_error_ens.pkl', 'wb') as fb:\n",
    "    pkl.dump(total_mass_error, fb)\n",
    "with open('total_mass_ens.pkl', 'wb') as fb:\n",
    "    pkl.dump(total_mass, fb)\n",
    "with open('cumulative_mass_all_ens.pkl', 'wb') as fb:\n",
    "    pkl.dump(cumulative_mass_all, fb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mass_basin_list_ens.pkl', 'wb') as fb:\n",
    "    pkl.dump(mass_basin_list, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mass_basin_list['time_split1']))\n",
    "print(len(mass_basin_list['time_split2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing = pd.read_csv(forcing_dir+basin_0str+'_lump_{}_forcing_leap.txt'.format(file_name_map[forcing_type]), \n",
    "                      delim_whitespace=True, header=3)\n",
    "# if tsplt == 'time_split1':\n",
    "#     forcing = forcing.iloc[3560:6848]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Mnth</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hr</th>\n",
       "      <th>Dayl(s)</th>\n",
       "      <th>PRCP(mm/day)</th>\n",
       "      <th>SRAD(W/m2)</th>\n",
       "      <th>SWE(mm)</th>\n",
       "      <th>Tmax(C)</th>\n",
       "      <th>Tmin(C)</th>\n",
       "      <th>Vp(Pa)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3560</th>\n",
       "      <td>1989</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>41472.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>321.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.12</td>\n",
       "      <td>10.12</td>\n",
       "      <td>857.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3561</th>\n",
       "      <td>1989</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>41126.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>234.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.76</td>\n",
       "      <td>7.76</td>\n",
       "      <td>729.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3562</th>\n",
       "      <td>1989</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>41126.40</td>\n",
       "      <td>3.56</td>\n",
       "      <td>231.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.06</td>\n",
       "      <td>11.06</td>\n",
       "      <td>1166.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3563</th>\n",
       "      <td>1989</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>40780.80</td>\n",
       "      <td>13.40</td>\n",
       "      <td>298.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.46</td>\n",
       "      <td>16.46</td>\n",
       "      <td>1618.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3564</th>\n",
       "      <td>1989</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>40665.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>202.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.59</td>\n",
       "      <td>5.59</td>\n",
       "      <td>590.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7209</th>\n",
       "      <td>1999</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>41817.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>406.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.44</td>\n",
       "      <td>12.44</td>\n",
       "      <td>967.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210</th>\n",
       "      <td>1999</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>41817.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>391.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.09</td>\n",
       "      <td>13.09</td>\n",
       "      <td>1175.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7211</th>\n",
       "      <td>1999</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>41472.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>369.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.21</td>\n",
       "      <td>13.21</td>\n",
       "      <td>1282.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7212</th>\n",
       "      <td>1999</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>41472.00</td>\n",
       "      <td>5.03</td>\n",
       "      <td>300.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.14</td>\n",
       "      <td>15.14</td>\n",
       "      <td>1525.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7213</th>\n",
       "      <td>1999</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>41126.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>411.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.97</td>\n",
       "      <td>12.97</td>\n",
       "      <td>1081.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3654 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Mnth  Day  Hr   Dayl(s)  PRCP(mm/day)  SRAD(W/m2)  SWE(mm)  \\\n",
       "3560  1989     9   30  12  41472.00          0.00      321.20      0.0   \n",
       "3561  1989    10    1  12  41126.40          0.00      234.58      0.0   \n",
       "3562  1989    10    2  12  41126.40          3.56      231.19      0.0   \n",
       "3563  1989    10    3  12  40780.80         13.40      298.96      0.0   \n",
       "3564  1989    10    4  12  40665.19          0.00      202.54      0.0   \n",
       "...    ...   ...  ...  ..       ...           ...         ...      ...   \n",
       "7209  1999     9   27  12  41817.60          0.00      406.93      0.0   \n",
       "7210  1999     9   28  12  41817.60          0.00      391.55      0.0   \n",
       "7211  1999     9   29  12  41472.00          0.00      369.92      0.0   \n",
       "7212  1999     9   30  12  41472.00          5.03      300.31      0.0   \n",
       "7213  1999    10    1  12  41126.40          0.00      411.33      0.0   \n",
       "\n",
       "      Tmax(C)  Tmin(C)   Vp(Pa)  \n",
       "3560    10.12    10.12   857.80  \n",
       "3561     7.76     7.76   729.34  \n",
       "3562    11.06    11.06  1166.67  \n",
       "3563    16.46    16.46  1618.25  \n",
       "3564     5.59     5.59   590.40  \n",
       "...       ...      ...      ...  \n",
       "7209    12.44    12.44   967.46  \n",
       "7210    13.09    13.09  1175.11  \n",
       "7211    13.21    13.21  1282.06  \n",
       "7212    15.14    15.14  1525.88  \n",
       "7213    12.97    12.97  1081.10  \n",
       "\n",
       "[3654 rows x 11 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forcing.iloc[3560:7214]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
